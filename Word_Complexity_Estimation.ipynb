{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aHcuLTPKqoLE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ],
      "id": "aHcuLTPKqoLE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf882e5-85e8-46b4-87e4-cebb154ff018"
      },
      "source": [
        "### MISC functions"
      ],
      "id": "ecf882e5-85e8-46b4-87e4-cebb154ff018"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "73aafe5e-8a69-4bf0-a98c-b4363126b0f3"
      },
      "outputs": [],
      "source": [
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    count = 0\n",
        "    vowels = \"aeiouy\"\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for index in range(1, len(word)):\n",
        "        if word[index] in vowels and word[index - 1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(\"e\"):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count"
      ],
      "id": "73aafe5e-8a69-4bf0-a98c-b4363126b0f3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "606b4bfa-da6a-4175-8bc5-5a191aa767d6"
      },
      "outputs": [],
      "source": [
        "def check_consonant(x): \n",
        "      \n",
        "    # convert string in lowercase\n",
        "    x = x.lower() \n",
        "  \n",
        "    return not (x == 'a' or x == 'e' or \n",
        "                x == 'i' or x == 'o' or \n",
        "                x == 'u') \n",
        "\n",
        "# function to count consonants  \n",
        "def countConsonants(string): \n",
        "      \n",
        "    count = 0\n",
        "      \n",
        "    for i in range(len(string)): \n",
        "  \n",
        "        # To check is character is Consonant \n",
        "        if (check_consonant(string[i])): \n",
        "            count += 1\n",
        "              \n",
        "    return count "
      ],
      "id": "606b4bfa-da6a-4175-8bc5-5a191aa767d6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b96c0ce8-4e36-4722-ba16-19ac82701933"
      },
      "outputs": [],
      "source": [
        "def remove_noise(text):\n",
        "    x=text.split()\n",
        "    if(len(x)==1):\n",
        "        new = text.replace('\"','').replace(':','').replace('“','').replace('”','').replace('-',' ').replace('—','').replace(')','').replace('(','').replace(\".\",\"\").replace(\",\",\"\").replace(\"€\",\"\").replace(\"$\",\"\").replace(\"£\",\"\").replace(\"—\",\"\") .replace(\"’\",\"\").replace(\"→\",\"\").replace(\"%\",\"\").replace(\";\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"/\",\"\")\n",
        "        return new\n",
        "    new_text=[]\n",
        "    for word in x:\n",
        "        new_text.append(word.replace('\"','').replace(':','').replace('“','').replace('”','').replace('-',' ').replace('—','').replace(')','').replace('(','').replace(\".\",\"\").replace(\",\",\"\").replace(\"€\",\"\").replace(\"$\",\"\").replace(\"£\",\"\").replace(\"—\",\"\") .replace(\"’\",\"\").replace(\"→\",\"\").replace(\"%\",\"\").replace(\";\",\"\").replace(\"]\",\"\").replace(\"[\",\"\").replace(\"/\",\"\"))\n",
        "\n",
        "    return \" \".join(new_text)"
      ],
      "id": "b96c0ce8-4e36-4722-ba16-19ac82701933"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9575a1d1-f3a2-4961-850e-fc918efd29e1"
      },
      "outputs": [],
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]"
      ],
      "id": "9575a1d1-f3a2-4961-850e-fc918efd29e1"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "42bcb21a-f99d-4577-b6ac-1100410201f6"
      },
      "outputs": [],
      "source": [
        "def word_gst(matrix, word):\n",
        "    for pair in matrix:\n",
        "        if pair[0] == word:\n",
        "            return pair[1]"
      ],
      "id": "42bcb21a-f99d-4577-b6ac-1100410201f6"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "710e7cfd-e269-4478-97a0-82c3d1e118c3"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
        "            result.append(token)\n",
        "\n",
        "    return result\n"
      ],
      "id": "710e7cfd-e269-4478-97a0-82c3d1e118c3"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FzGVotE6oAvT"
      },
      "outputs": [],
      "source": [
        "def get_features(data, dictionary):\n",
        "    dict_keys = list(dictionary.keys())\n",
        "    ft_mx = []\n",
        "    for sentence in data:\n",
        "        x=np.zeros(len(dict_keys))\n",
        "        for word in sentence.split():\n",
        "            if word in dict_keys:\n",
        "                x[dict_keys.index(word)] +=1\n",
        "        ft_mx.append(x)\n",
        "    ft_mx=np.array(ft_mx)\n",
        "\n",
        "    return ft_mx\n"
      ],
      "id": "FzGVotE6oAvT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WUivRE8p2NC"
      },
      "source": [
        "### Main Functions\n",
        "Every sentecnce is extracted from the input file and is added to a new dataframe.The total complexity of that sentence, all the words concatenanted in a string and all the gold standards concatenanted in a string are appended to the new transformed dataframe."
      ],
      "id": "9WUivRE8p2NC"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d19ae592-594e-426c-8d4a-c94b4d0049f0"
      },
      "outputs": [],
      "source": [
        "def transform_df(df, train_flag):\n",
        "    if train_flag:\n",
        "        df.drop(df.loc[df['offset_start']==1].index, inplace=True) #drop rows with offset start at 1\n",
        "    \n",
        "    #columns\n",
        "    sentences = []\n",
        "    total_gs = [] #gs=gold standard\n",
        "    all_tw = []   #tw=target words\n",
        "    all_gs = []\n",
        "\n",
        "    #accumulators\n",
        "    tw =[]\n",
        "    gold_std = []\n",
        "    gs_acc = 0\n",
        "    temp = df['sentence'][df.first_valid_index()]\n",
        "    nr = 0\n",
        "    count = 0\n",
        "    for idx in df.index:\n",
        "        if temp != df['sentence'][idx]:\n",
        "            sentences.append(temp)\n",
        "            all_tw.append(\"|\".join(map(str, tw))) #create a string from the list\n",
        "            \n",
        "            #if train_flag:\n",
        "            total_gs.append(gs_acc)  \n",
        "            all_gs.append(\" \".join(map(str, gold_std)))\n",
        "            gold_std.clear()\n",
        "            gs_acc = 0\n",
        "                \n",
        "            tw.clear()\n",
        "            temp = df['sentence'][idx]\n",
        "\n",
        "        tw.append(df['word'][idx])\n",
        "        \n",
        "        gs_acc += df['gold_std'][idx]\n",
        "        gold_std.append(df['gold_std'][idx])\n",
        "\n",
        "\n",
        "    sentences.append(temp)\n",
        "    all_tw.append(\"|\".join(map(str, tw))) #create a string from the list\n",
        "    #print(all_tw)\n",
        "\n",
        "    #if train_flag:\n",
        "    total_gs.append(gs_acc)  \n",
        "    all_gs.append(\" \".join(map(str, gold_std)))\n",
        "#if train_flag:\n",
        "    #print(all_tw)\n",
        "    return pd.DataFrame(list(zip(sentences, total_gs, all_tw, all_gs)), columns = ['sentence', 'gold_std', 'target_words', 'gold_stds']) #the new columns for the dataframe\n",
        "    #else:\n",
        "        #return pd.DataFrame(list(zip(sentences, all_tw)), columns = ['sentence', 'target_words'])"
      ],
      "id": "d19ae592-594e-426c-8d4a-c94b4d0049f0"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "987f5d74-97eb-4c87-a46c-510e0fd67fb5"
      },
      "outputs": [],
      "source": [
        "def extract_features(input_file, train_flag):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    twords_lem = [] #all the target words processed and lemmatized, ready to be matched with the dictionary based on he sentences\n",
        "    eligible_sentences = []\n",
        "    twords_gstd_all = [] #matrix of processed target words with their gold standard\n",
        "    upd_tw = [] #target_words\n",
        "    upd_gs = [] #target words gold standard\n",
        "    for idx,row in input_file.iterrows(): # for every row from the grouped df\n",
        "        temp = row[0]\n",
        "\n",
        "        tokenized_sentence = nlp(row[0])#remove noise from the sentence\n",
        "        #decode words and gold standard\n",
        "        twords = row['target_words'].split('|')#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#original target words\n",
        "        #print(twords)\n",
        "        #if train_flag:\n",
        "        gs_word = row['gold_stds'].split()\n",
        "        twords_gstd = [[tw, gs] for tw,gs in zip(twords, gs_word)]  ###('America',0.1)\n",
        "        #temp = set([remove_noise(tgroup) for tgroup in twords]) #remove noise from every target group/word ######################################################################## TODO nu elimina noise-ul aici ca sa nu alterezi contextul\n",
        "\n",
        "        eligible_words=[] # all processed words from the sentence (used to create lematized text)\n",
        "        sent = \"\"\n",
        "        twords_sentence_lem =[] #only processed words which are on 'word' col\n",
        "        twords_sentence_unlem = [] #unlematized version of the lematized words\n",
        "        groups_left = [] #group of words which won't be processed by context\n",
        "\n",
        "        twg_lem=[]\n",
        "        temp_twg_lem = []\n",
        "        ######################################################## (procesat doar target words si gold_std) PANA AICI am extras toate grupurile de (word,gold_std) disponibile in tot datasetul din cele date la target words si am creat un set cu aceste target words\n",
        "\n",
        "        #######################################################  (urmeaza procesarea propozitiei)\n",
        "        twords_trace = twords.copy()\n",
        "        for token in tokenized_sentence: #iau fiecare cuvant din propozitie\n",
        "            ini_word = str(token) #save the unlematized word\n",
        "            ################################################### (salvez in forma lemma lowercase toate cuvintele din propozitie care nu sunt stop words\n",
        "            if not(token.is_stop) and not (any(char.isdigit() for char in str(token))) and not(str(token).isspace()) and len(str(token.lemma_))>1: #remove stop words and strings which contain digits and words of one letter\n",
        "                token_lema = token.lemma_\n",
        "                eligible_words.append(str(token_lema).lower()) #add every lemmatized word \n",
        "\n",
        "                ############################################### (salvez in temp_twg_lem cuvantul in forma lemma lowercase daca este target word, impreuna cu gold_std\n",
        "                if ini_word in twords_trace: #if the unlematized word is in the unlematized word col\n",
        "                    #print(\"*tt:\",len(twords_trace)) \n",
        "                    twords_trace.remove(ini_word)\n",
        "                    #print(\"**tt:\",len(twords_trace))\n",
        "                    \n",
        "                    #print(twords_trace)\n",
        "                    if train_flag:\n",
        "                        #print(token_lema.lower())\n",
        "                        temp_twg_lem.append([ float(row['gold_std']), ini_word, token_lema.lower(), word_gst(twords_gstd, ini_word)]) #create new matrix with the lemmatized word and its gold standard\n",
        "                        #print(\"*\",token_lema.lower())\n",
        "                        upd_gs.append(word_gst(twords_gstd, ini_word))\n",
        "                    else:\n",
        "                        temp_twg_lem.append([row[0], ini_word, token_lema.lower(), word_gst(twords_gstd, ini_word)])\n",
        "                    upd_tw.append(token_lema.lower())\n",
        "                    twords_sentence_lem.append(token_lema.lower()) #then add the contextual lematized version to twords_proc\n",
        "                    twords_sentence_unlem.append(str(ini_word))\n",
        "            ################################################### (daca cuvantul din propozite este stop_word si se afla in target words, atunci salvez perechea in temp_twg_lem si in vocabular\n",
        "            #                                                    SAU daca cuvantul\n",
        "            elif token.is_stop and len(str(token.lemma_))>1:# and (str(token))[0].isupper(:\n",
        "                token_lema = token.lemma_\n",
        "                eligible_words.append(str(token_lema).lower()) #add every lemmatized word\n",
        "                if ini_word in twords: #if the unlematized word is in the unlematized word col\n",
        "                    #print(\"*tt:\",len(twords_trace)) \n",
        "                    twords_trace.remove(ini_word)\n",
        "                    #print(\"**tt:\",len(twords_trace))\n",
        "                    if train_flag:\n",
        "                        #print(token_lema.lower())\n",
        "                        temp_twg_lem.append([ float(row['gold_std']), ini_word, token_lema.lower(), word_gst(twords_gstd, ini_word)]) #*\n",
        "                        #print(\"**\",token_lema.lower())\n",
        "                        upd_gs.append(word_gst(twords_gstd, ini_word))\n",
        "                    else:\n",
        "                        temp_twg_lem.append([row[0], ini_word, token_lema.lower(), word_gst(twords_gstd, ini_word)])\n",
        "                    upd_tw.append(token_lema.lower())\n",
        "                    twords_sentence_lem.append(token_lema.lower()) #then add the contextual lematized version to twords_proc\n",
        "                    twords_sentence_unlem.append(str(ini_word))\n",
        "\n",
        "        #process group of words that match 'word' column\n",
        "        ################################################### (compar numarul de cuvinte adaugate in matrice cu cele din target words; ar trebui sa iasa in plus grupurile din target words\n",
        "\n",
        "        if len(twords_sentence_unlem) != len(twords):\n",
        "            groups_left = list(set(twords)^set(twords_sentence_unlem))\n",
        "            '''\n",
        "            print(\"tsu: \",twords_sentence_unlem)\n",
        "            print(\"tw:  \",twords)\n",
        "            print(\"gl:  \",groups_left)\n",
        "            print(\"\")\n",
        "            '''\n",
        "            \n",
        "            for group in groups_left:\n",
        "                #if group == '-':\n",
        "                #    continue\n",
        "                twords_trace.remove(str(group))\n",
        "                lem_group=[]\n",
        "                tok_group = nlp(group)\n",
        "                for word in tok_group:\n",
        "                    if str(word) == '-' or str(word) == '—':\n",
        "                        continue\n",
        "                    lem_group.append(str(word.lemma_).lower())\n",
        "                if train_flag:\n",
        "                    #print(lem_group)\n",
        "                    temp_twg_lem.append([float(row['gold_std']), group, \" \".join(lem_group), word_gst(twords_gstd, group)]) #add to the matrix the group of words################################ (adaug in matrice grupul de cuvinte cu cuvintele lematizate individual)\n",
        "                    upd_gs.append(word_gst(twords_gstd, ini_word))\n",
        "                else:\n",
        "                    temp_twg_lem.append([row[0], group, \" \".join(lem_group), word_gst(twords_gstd, group)])\n",
        "                twords_sentence_lem.append(\" \".join(lem_group)) #add the local lematized group to the lematized target words array\n",
        "                upd_tw.append(lem_group)\n",
        "\n",
        "        ################################################### (daca numarul de target words e diferit de numarul de elemente din matricea mea)\n",
        "        twg_lem = temp_twg_lem\n",
        "        elig_sent = \" \".join(eligible_words) ###############creez propozitie cu cuvintele eligibile\n",
        "        eligible_sentences.append(remove_noise(elig_sent)) #mai elimin noise-ul (redundant)\n",
        "        twords_lem.append(twords_sentence_lem) ########adaug propozitia eligibila la array-ul general de cuvinte\n",
        "        \n",
        " ########################################################################## \n",
        "        if(len(twords)!= len(twg_lem)): #for the case when a target word appears more than the appearances in the sentence\n",
        "            '''print(\"twords:  \",twords)\n",
        "            x=[el[-3] for el in twg_lem]\n",
        "            print(\"twg_lem: \",x)\n",
        "            print(\"rest:    \",twords_trace)'''\n",
        "            \n",
        "            for el in twords_trace:\n",
        "                lem_group=[]\n",
        "                tok_group = nlp(el)\n",
        "                for word in tok_group:\n",
        "                    if str(word) == '-' or str(word) == '—':\n",
        "                        continue\n",
        "                    lem_group.append(str(word.lemma_).lower())\n",
        "                if len(el.split()) > 1:\n",
        "                    if train_flag:\n",
        "                        temp_twg_lem.append([float(row['gold_std']), str(el), \" \".join(lem_group), word_gst(twords_gstd, el)]) #add to the matrix the group of words################################ (adaug in matrice grupul de cuvinte cu cuvintele lematizate individual)\n",
        "                        upd_gs.append(word_gst(twords_gstd, el))\n",
        "                    else:\n",
        "                        temp_twg_lem.append([row[0], str(el), \" \".join(lem_group), word_gst(twords_gstd, el)])\n",
        "                    twords_sentence_lem.append(\" \".join(lem_group)) #add the local lematized group to the lematized target words array\n",
        "                else:\n",
        "                    if train_flag:\n",
        "                        temp_twg_lem.append([float(row['gold_std']), str(el), str(lem_group), word_gst(twords_gstd, el)]) #add to the matrix the group of words################################ (adaug in matrice grupul de cuvinte cu cuvintele lematizate individual)\n",
        "                        upd_gs.append(word_gst(twords_gstd, el))\n",
        "                    else:\n",
        "                        temp_twg_lem.append([row[0], str(el), str(lem_group), word_gst(twords_gstd, el)])\n",
        "                    twords_sentence_lem.append(str(lem_group)) #add the local lematized group to the lematized target words array\n",
        "                upd_tw.append(str(lem_group))\n",
        "  ######################################################################################                      \n",
        "        twords_gstd_all.append(twg_lem)        ########adauga matricea la matricea generala de (target word, gold_std)\n",
        "        \n",
        "        input_file.at[idx, 'sentence'] = elig_sent\n",
        "        joined_upd_tw = []\n",
        "        for el in upd_tw:\n",
        "            if type(el) == list:\n",
        "                joined_upd_tw.append(\" \".join(el))\n",
        "            else:\n",
        "                joined_upd_tw.append(el)\n",
        "        input_file.at[idx, 'target_words'] = \"|\".join(map(str, joined_upd_tw)) #create a string from the list\n",
        "        joined_upd_tw.clear()\n",
        "        upd_tw.clear()\n",
        "        upd_gs.clear()\n",
        "        \n",
        "    ### finished iterating through dataset\n",
        "    \n",
        "    ########################################\n",
        "    eligible_text = \" \".join(input_file['sentence'].values) #all the distinct sentences without punctuation, and strings with digits\n",
        "    temp = flatten(twords_lem)\n",
        "    twords_lem = temp\n",
        "\n",
        "    words_elig_sent=\" \".join(eligible_sentences).split()\n",
        "    \n",
        "    for el in twords_lem:\n",
        "        if el not in words_elig_sent:\n",
        "            if len(el.split()) == 1:\n",
        "                words_elig_sent.append(el)\n",
        "    eligible_text = \" \".join(words_elig_sent)\n",
        "\n",
        "    temp = flatten(twords_gstd_all)\n",
        "    twords_gst_all = temp\n",
        "    ########################################\n",
        "    eligible_text = \" \".join(input_file['sentence'].values) #all the distinct sentences without punctuation, and strings with digits\n",
        "    temp = flatten(twords_lem)\n",
        "    words_elig_sent=\" \".join(eligible_sentences).split()\n",
        "\n",
        "\n",
        "            \n",
        "    for el in temp:\n",
        "        if el not in words_elig_sent:\n",
        "            if len(el.split()) == 1:\n",
        "                words_elig_sent.append(el)\n",
        "    eligible_text = \" \".join(words_elig_sent)\n",
        "\n",
        "    temp = flatten(twords_gstd_all)\n",
        "    twords_gst_all = temp\n",
        "\n",
        "    return flatten(twords_gstd_all), input_file, eligible_text"
      ],
      "id": "987f5d74-97eb-4c87-a46c-510e0fd67fb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bf849d-fb43-4a8b-9dba-e3414655c812"
      },
      "source": [
        "# Preprocess data"
      ],
      "id": "a2bf849d-fb43-4a8b-9dba-e3414655c812"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1b3lQQXtJIF"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install requests-html"
      ],
      "id": "C1b3lQQXtJIF"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10851727-1b6f-409d-9e75-0788afa836d5",
        "outputId": "e55b0c2d-66d4-4a92-fcf1-fc064eb98f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "source": [
        "file = pd.read_csv('/content/sample_data/train_full.csv')\n",
        "testfile = pd.read_csv('/content/sample_data/test.csv')\n",
        "\n",
        "train_data = file.iloc[:11017]\n",
        "test_data = file.iloc[11017:]\n",
        "\n",
        "#Extract the sentence the total gold_std for every words labeled from every sentence, the labeled words and their gold standard\n",
        "fabr_gstd = [0] * len(testfile.index)\n",
        "testfile['gold_std'] = fabr_gstd\n",
        "\n",
        "transformed_train = transform_df(train_data, train_flag = True)\n",
        "transformed_official_test = transform_df(test_data, train_flag = False)\n",
        "\n",
        "twords_gstd_all, train_file, eligible_text = extract_features(transformed_train, train_flag = True)\n",
        "twords_gstd_official_test, new_testfile, eligible_text_official = extract_features(transformed_official_test, train_flag = False)\n",
        "twords_gstd_all_test = twords_gstd_official_test\n",
        "\n",
        "real_gstd=test_data['gold_std']"
      ],
      "id": "10851727-1b6f-409d-9e75-0788afa836d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fc8f07c-4cbc-44aa-8f6c-7c877316d5a6"
      },
      "source": [
        "# Features"
      ],
      "id": "2fc8f07c-4cbc-44aa-8f6c-7c877316d5a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18c0afe1-dc98-46e7-ab41-7c689b498417"
      },
      "source": [
        " ### 1)Character N-grams (2,3,4 grams)"
      ],
      "id": "18c0afe1-dc98-46e7-ab41-7c689b498417"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4605627-79e2-49d5-984f-b2c3e0e0ec5c",
        "outputId": "e93e13cd-370f-4038-f6ff-af2290ebe47c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "763\n"
          ]
        }
      ],
      "source": [
        "tw = [el[1] for el in twords_gstd_all] #all target words\n",
        "tw_test = [el[1] for el in twords_gstd_all_test]\n",
        "four_g = []\n",
        "three_g = []\n",
        "two_g = []\n",
        "four_g.clear()\n",
        "three_g.clear()\n",
        "two_g.clear()\n",
        "for word in tw:\n",
        "    two_g.append([ word[i:i+2] for i in range(len(word)-1) if len(word[i:i+2]) == 2 and \" \" not in word[i:i+2]])\n",
        "    four_g.append([ word[i:i+4] for i in range(len(word)-1) if len(word[i:i+4]) == 4 and \" \" not in word[i:i+4]])\n",
        "    \n",
        "bg_sent = [\" \".join(el) for el in two_g]\n",
        "bg_dict = dict(Counter(flatten(two_g)))\n",
        "ng_tw = [flatten([fourg, twog]) for fourg, twog in zip(four_g, two_g)]\n",
        "ng_sent = [\" \".join(el) for el in ng_tw]\n",
        "ng_dict = dict(Counter(flatten(ng_tw)))\n",
        "print(len(bg_dict))"
      ],
      "id": "f4605627-79e2-49d5-984f-b2c3e0e0ec5c"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZAByfhECpoZQ"
      },
      "outputs": [],
      "source": [
        "################################################################################# test set\n",
        "tw_test = [el[1] for el in twords_gstd_all_test]\n",
        "four_g.clear()\n",
        "three_g.clear()\n",
        "two_g.clear()\n",
        "c=0\n",
        "for word in tw_test:\n",
        "    c+=1\n",
        "    four_g.append([ word[i:i+4] for i in range(len(word)-1) if len(word[i:i+4]) == 4 and \" \" not in word[i:i+4]])\n",
        "    two_g.append([ word[i:i+2] for i in range(len(word)-1) if len(word[i:i+2]) == 2 and \" \" not in word[i:i+2]])\n",
        "    \n",
        "#ng_tw_test = [flatten([fourg, threeg, twog]) for fourg, threeg, twog in zip(four_g, three_g, two_g)]\n",
        "ng_tw_test = [flatten([fourg, twog]) for fourg, twog in zip(four_g, two_g)]\n",
        "#ng_tw_test = [flatten([fourg, threeg]) for fourg, threeg in zip(four_g, three_g)]\n",
        "ng_sent_test = [\" \".join(el) for el in ng_tw_test]"
      ],
      "id": "ZAByfhECpoZQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2UotY3etkiV"
      },
      "source": [
        "### 2) Word present in news dump - feature"
      ],
      "id": "C2UotY3etkiV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpYqhvFNjblJ"
      },
      "outputs": [],
      "source": [
        "!pip install newspaper3k"
      ],
      "id": "mpYqhvFNjblJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRYuMppxtkPK"
      },
      "outputs": [],
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "pagesToGet= 50    #read 50 news articles\n",
        "article_links = []\n",
        "upperframe=[]  \n",
        "for page in range(1,pagesToGet+1):\n",
        "    print('processing page :', page)\n",
        "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
        "    print(url)\n",
        "    \n",
        "    #an exception might be thrown, so the code should be in a try-except block\n",
        "    try:\n",
        "        #use the browser to get the url. This is suspicious command that might blow up.\n",
        "        page=requests.get(url)                             # this might throw an exception if something goes wrong.\n",
        "    \n",
        "    except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "        error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "        print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
        "        print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "        continue                                              #ignore this page. Abandon this and go back.\n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(page.text,'html.parser')\n",
        "    frame=[]\n",
        "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
        "    print(len(links))\n",
        "    \n",
        "    for j in links:\n",
        "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
        "        #print(Statement)\n",
        "        Link = \"https://www.politifact.com\"\n",
        "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
        "        article_links.append(Link)"
      ],
      "id": "fRYuMppxtkPK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB3owlwGlGou"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from newspaper import Article\n",
        "\n",
        "topic_texts = []\n",
        "preprocessed_files = []\n",
        "\n",
        "articles = ['https://www.politifact.com/factchecks/2021/dec/22/facebook-posts/sealed-room-would-snake-plants-keep-you-alive-stud/','https://www.politifact.com/factchecks/2021/dec/22/facebook-posts/sealed-room-would-snake-plants-keep-you-alive-stud/']\n",
        "c=0\n",
        "for article_link in article_links:\n",
        "    print(c)\n",
        "    c+=1\n",
        "    article = Article(url=\"%s\" % article_link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    paragraphs = article.text.split(\"\\n\")\n",
        "    #print(paragraphs)\n",
        "    for i,el in enumerate(paragraphs):\n",
        "        if \"We rate this\" in el:\n",
        "            paragraphs.remove(el)\n",
        "    par_list = \"\\n\".join(paragraphs)\n",
        "    pp_text = preprocess(par_list)  #stem every word and remove special characters                                                               \n",
        "    txt = \" \".join(pp_text)   #use the first 100 eligibile words from every wikipedia page\n",
        "    topic_texts.append(txt.split())\n",
        "temp = topic_texts.copy()\n",
        "preprocessed_files.append(temp)       #save all preprocessed texts in preprocesed_files"
      ],
      "id": "GB3owlwGlGou"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7wptHy-t7Xk"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "f = open(\"news_.txt\", \"r\", encoding=\"utf-8\")\n",
        "all_articles = f.read().split(\"\\n\")\n",
        "temp = [ article for article in all_articles if article!= \"\"]\n",
        "all_articles = temp\n",
        "\n",
        "all_articles_wlist = [article.split() for article in all_articles]\n",
        "article_dictionary = dict(Counter(flatten(all_articles_wlist)))\n",
        "#print(len(article_dictionary))\n",
        "print(len(flatten(all_articles_wlist)))\n",
        "print(len(article_dictionary))\n",
        "top_dict = sorted(article_dictionary.items(), key=lambda x: x[1], reverse = True)\n",
        "common_words = [key[0] for key in top_dict[:len(article_dictionary)//10]]\n",
        "print(common_words)"
      ],
      "id": "R7wptHy-t7Xk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTohzQ9euWiP"
      },
      "outputs": [],
      "source": [
        "news_dump_presence_train = []\n",
        "news_dump_presence_test = []\n",
        "\n",
        "for el in tw:\n",
        "    if el in common_words:\n",
        "        news_dump_presence_train.append(1)\n",
        "    else:\n",
        "        news_dump_presence_train.append(0)\n",
        "        \n",
        "for el in tw_test:\n",
        "    if el in common_words:\n",
        "        news_dump_presence_test.append(1)\n",
        "    else:\n",
        "        news_dump_presence_test.append(0)\n",
        "additional_ftrs_train.append(np.array(news_dump_presence_train))\n",
        "additional_ftrs_test.append(np.array(news_dump_presence_test))\n",
        "\n",
        "print(len(additional_ftrs_train))\n",
        "print(len(additional_ftrs_test))"
      ],
      "id": "hTohzQ9euWiP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRv9Pvv4uNkz"
      },
      "source": [
        "### 3) Numeric feature"
      ],
      "id": "VRv9Pvv4uNkz"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tijYXMGzuOgP"
      },
      "outputs": [],
      "source": [
        "additional_ftrs_train = []\n",
        "additional_ftrs_test = []\n",
        "\n",
        "alpha_train = []\n",
        "alpha_test = []\n",
        "\n",
        "for word in tw:\n",
        "    if word.isalpha(): #contains only letters\n",
        "        alpha_train.append(1)\n",
        "    else:\n",
        "        alpha_train.append(0)\n",
        "for word in tw_test:\n",
        "    if word.isalpha(): #contains only letters\n",
        "        alpha_test.append(1)\n",
        "    else:\n",
        "        alpha_test.append(0)\n",
        "\n",
        "additional_ftrs_train.append(np.array(alpha_train))\n",
        "additional_ftrs_test.append(np.array(alpha_test))\n"
      ],
      "id": "tijYXMGzuOgP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RG-DOi8ucJm"
      },
      "source": [
        "### 4) word Length - feature"
      ],
      "id": "-RG-DOi8ucJm"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kaoIAJayud4z"
      },
      "outputs": [],
      "source": [
        "character_count_train = [len(string) for string in tw]\n",
        "character_count_test = [len(string) for string in tw_test]\n",
        "\n",
        "additional_ftrs_train.append(np.array(character_count_train))\n",
        "additional_ftrs_test.append(np.array(character_count_test))"
      ],
      "id": "kaoIAJayud4z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "282ffed2-6223-41d7-8559-2f2510d1e70b"
      },
      "source": [
        "### 5) sylabble count"
      ],
      "id": "282ffed2-6223-41d7-8559-2f2510d1e70b"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "8453666d-9cb3-4302-97e5-f94be37a0a93"
      },
      "outputs": [],
      "source": [
        "syl_count = []\n",
        "for word in tw:\n",
        "    syl_count.append(syllable_count(word))"
      ],
      "id": "8453666d-9cb3-4302-97e5-f94be37a0a93"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb941f8-08b4-4d78-b683-d29612135e2b"
      },
      "source": [
        "### 6) contains number\n"
      ],
      "id": "afb941f8-08b4-4d78-b683-d29612135e2b"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3d8815cc-39f5-4da3-b150-bfc7c7ab91e5"
      },
      "outputs": [],
      "source": [
        "alpha = []\n",
        "for word in tw:\n",
        "    if word.isalpha(): #contains only letters\n",
        "        alpha.append(1)\n",
        "    else:\n",
        "        alpha.append(0)"
      ],
      "id": "3d8815cc-39f5-4da3-b150-bfc7c7ab91e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8bc2e34-52a5-492a-8265-afc9f3cdf3c7"
      },
      "source": [
        "### 7) Multi word expression"
      ],
      "id": "a8bc2e34-52a5-492a-8265-afc9f3cdf3c7"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "dcfa9a20-e79c-48f3-83fb-d0375b7d9271"
      },
      "outputs": [],
      "source": [
        "mwe = []\n",
        "\n",
        "for string in tw:\n",
        "    if \" \" in string:             #is multi word expression\n",
        "        mwe.append(1)\n",
        "    else:\n",
        "        mwe.append(0)"
      ],
      "id": "dcfa9a20-e79c-48f3-83fb-d0375b7d9271"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1c342be-61a0-4ec1-bc79-1c10f1e02916"
      },
      "source": [
        "### 8) number of characters"
      ],
      "id": "d1c342be-61a0-4ec1-bc79-1c10f1e02916"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8beab4b-9532-475d-83b3-0e86e615b90d"
      },
      "outputs": [],
      "source": [
        "\n",
        "character_count = [len(string) for string in tw]\n",
        "print(character_count)"
      ],
      "id": "d8beab4b-9532-475d-83b3-0e86e615b90d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aNAp778n4Tn"
      },
      "source": [
        "### Create feature matrix"
      ],
      "id": "8aNAp778n4Tn"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2qJCep0Vn1k2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9366c2f9-7bff-4f03-aa00-e9827f377db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2985,)\n",
            "(2985,)\n",
            "(11014, 6641)\n",
            "(2985, 6641)\n",
            "(11014,)\n"
          ]
        }
      ],
      "source": [
        "###################################### Extracting features\n",
        "nb_labels = np.array([el[3] for el in twords_gstd_all])\n",
        "real_gstd_train = np.array([el[3] for el in twords_gstd_all_test])\n",
        "\n",
        "nb_train_ft_mx = get_features(ng_sent, ng_dict)\n",
        "for feature in additional_ftrs_train: \n",
        "    nb_train_ft_mx = np.insert(nb_train_ft_mx,0,feature,axis=1)\n",
        "\n",
        "\n",
        "nb_test_ft_mx = get_features(ng_sent_test, ng_dict)\n",
        "for feature in additional_ftrs_test:\n",
        "    print(feature.shape)\n",
        "    nb_test_ft_mx = np.insert(nb_test_ft_mx,0,feature,axis=1)\n",
        "print(nb_train_ft_mx.shape) #11014 target words and 9807 features (distinct n-grams)\n",
        "print(nb_test_ft_mx.shape) #11014 target words and 9807 features (distinct n-grams)\n",
        "print(nb_labels.shape)\n",
        "#print(new_nb_test_ft_mx.shape)\n",
        "\n",
        "\n",
        "#print(set(nb_pred))"
      ],
      "id": "2qJCep0Vn1k2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1a02_NGn1Ut"
      },
      "source": [
        "### Scaling"
      ],
      "id": "i1a02_NGn1Ut"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6t-527QqBXa",
        "outputId": "2e62b867-0c52-468c-b837-305bb75c14eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11014, 6641)\n",
            "(2985, 6641)\n"
          ]
        }
      ],
      "source": [
        "print(nb_train_ft_mx.shape)\n",
        "print(nb_test_ft_mx.shape)\n",
        "\n",
        "from sklearn import preprocessing \n",
        "scaler2 = preprocessing.Normalizer(norm='l2') \n",
        "scaler2.fit(nb_train_ft_mx)\n",
        "\n",
        "# scaling the training data\n",
        "scaled_features = scaler2.transform(nb_train_ft_mx)\n",
        "\n",
        "# scaling the test data\n",
        "scaled_test = scaler2.transform(nb_test_ft_mx)"
      ],
      "id": "r6t-527QqBXa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh7khdvLkdFT"
      },
      "source": [
        "# Classifiers"
      ],
      "id": "zh7khdvLkdFT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7STXcExkgKF"
      },
      "source": [
        "### Random Forest"
      ],
      "id": "_7STXcExkgKF"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va6TXTB5kvjb",
        "outputId": "fb5abee7-d9d1-40df-e5d4-80a0023b2409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 1 of 10\n",
            "building tree 2 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   37.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.7min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "(11014,)\n",
            "(2985,)\n",
            "RF\n",
            "mae:       0.11911853219818098\n",
            "max pred:  0.9879047619047618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.1min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "X = scaled_features\n",
        "X_test = scaled_test\n",
        "y = nb_labels\n",
        "temp = nb_labels.astype(np.float)\n",
        "y=temp\n",
        "\n",
        "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0, n_jobs = -1, verbose=10)\n",
        "  \n",
        "# fit the regressor with x and y data\n",
        "regressor.fit(X, y)  \n",
        "\n",
        "rf_pred = regressor.predict(X_test)\n",
        "\n",
        "real_gstd = real_gstd\n",
        "print(y.shape)\n",
        "print(rf_pred.shape)\n",
        "print(\"RF\")\n",
        "print(\"mae:      \",mae(real_gstd, rf_pred))\n",
        "print(\"max pred: \",max(rf_pred))"
      ],
      "id": "Va6TXTB5kvjb"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Word_Complexity_Estimation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
